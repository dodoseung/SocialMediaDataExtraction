{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YT\n",
    "from apiclient.discovery import build #pip install google-api-python-client\n",
    "from apiclient.errors import HttpError #pip install google-api-python-client\n",
    "from oauth2client.tools import argparser #pip install oauth2client\n",
    "from datetime import datetime\n",
    "import pandas as pd #pip install pandas\n",
    "import numpy as np\n",
    "import win32com.client\n",
    "import requests\n",
    "import json\n",
    "# + TW\n",
    "import time\n",
    "import sys\n",
    "import tweepy # pip install tweepy\n",
    "import urllib.request\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "# + YT comment_analytics\n",
    "from collections import Counter\n",
    "import operator\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords # pip install nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API를 사용하기 위한 key들\n",
    "if np.random.randint(1, 3, size=1)[0] == 1:\n",
    "    yt_api_key = \"AIzaSyDFx9O3qfqtT-j-VR9vqd9mSXOFGAB7Ick\" \n",
    "else:\n",
    "    yt_api_key = \"AIzaSyA6tLM4V5Aj90YXfCiQMnIv9dGq0kQMOIk\"\n",
    "\n",
    "tw_consumer_key=\"125Z3IxmGV1ZObDeIzDVvLyI0\"\n",
    "tw_consumer_secret=\"BxZCgYCnzE9OHrRH7NVUJUCob65p1RqypjHrpk5yseZCfuiC8A\"\n",
    "tw_access_token=\"249621931-dXFJt8Y5ZsGHAQwI8ygFsAHJHeh7vyDA93vKlGZZ\"\n",
    "tw_access_token_secret=\"thHCJMGNJV10iB83YDTbv2CG15pgTpAunGDslgyTBx9Ko\"\n",
    "tw_auth = tweepy.OAuthHandler(tw_consumer_key, tw_consumer_secret)\n",
    "tw_auth.set_access_token(tw_access_token, tw_access_token_secret)\n",
    "tw_api = tweepy.API(tw_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TWITTER\n",
    "\n",
    "def post_stat(url, user, timeline, num, row):\n",
    "       \n",
    "    favorite_count = []\n",
    "    retweet_count = []\n",
    "    reply_count = []\n",
    "    date = []\n",
    "    address = []\n",
    "    sa_count = []\n",
    "    sub_count = []\n",
    "    \n",
    "    for k in range(0,num):\n",
    "        try:\n",
    "            status = timeline[k]\n",
    "        except:\n",
    "            continue\n",
    "        numofpost = k + 1\n",
    "        \n",
    "        j_results = json.loads(json.dumps(status._json))   \n",
    "        sub_count.append(j_results['user']['followers_count'])\n",
    "        favorite_count.append(int(j_results['favorite_count']))\n",
    "        retweet_count.append(int(j_results['retweet_count']))\n",
    "        date.append(time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(j_results['created_at'],'%a %b %d %H:%M:%S +0000 %Y')))\n",
    "        if num == 1:\n",
    "            date = str(date)[2:12]\n",
    "            address.append(url.split('https://')[1])\n",
    "        else:\n",
    "            address.append('https://twitter.com/' + user.screen_name +'/status/' + j_results['id_str'])\n",
    "            #print('https://twitter.com/' + user.screen_name +'/status/' + j_results['id_str'])\n",
    "        \n",
    "        request = urllib.request.Request('https://twitter.com/' + str(j_results['user']['id']) + '/status/' + str(j_results['id']))\n",
    "        response = urllib.request.urlopen(request)\n",
    "        temp = response.read().decode('utf-8')\n",
    "        \n",
    "        try:\n",
    "            reply_count.append(int((temp.split('reply-count-aria-' + str(j_results['id']) + '\" data-aria-label-part>답글 ')[1].split('개')[0]).replace(',','')))\n",
    "        except:\n",
    "            reply_count.append(int(temp.split('reply-count-aria-' + str(j_results['id']) + '\" >답글 ')[1].split('개')[0]))\n",
    "            \n",
    "        sa_count.append(int(j_results['favorite_count']) + int(j_results['retweet_count']) + int(reply_count[k]))\n",
    "             \n",
    "    try:    \n",
    "        name = [user.screen_name] * numofpost\n",
    "    except:\n",
    "        name = [user]\n",
    "\n",
    "    dump = [0] * numofpost\n",
    "    Row = [row] * numofpost\n",
    "    \n",
    "    if num == 1:\n",
    "        final = pd.DataFrame(np.column_stack([name, date[:10], '', 'N/A', favorite_count, reply_count, retweet_count, address]), \n",
    "                         columns = ['name','post_date','UpdateDate','view_count','like_count','comment_count','share_count', 'url'])\n",
    "    else:\n",
    "        final = pd.DataFrame(np.column_stack([Row, dump, dump, sub_count, dump, dump, dump, dump, dump, dump, dump, sa_count]), \n",
    "                         columns = ['number','YT', 'IG', 'TW', 'FB', 'UpdateDate', 'View (YT)','View (IG)','YT SA','IG SA(Image)','IG SA(Video)','TW SA'])\n",
    "        final = final.groupby('number').mean()\n",
    "        \n",
    "    return final\n",
    "\n",
    "def comment_tw(url):\n",
    "    \n",
    "    try:\n",
    "        if url.rsplit('?',1)[1] != None:\n",
    "            url = url.rsplit('?',1)[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    text = []\n",
    "    name_list = []\n",
    "    \n",
    "    name = url.split('https://twitter.com/')[1].split('/')[0]\n",
    "    post_id = url.rsplit('/',1)[1]\n",
    "    \n",
    "    request = urllib.request.Request('https://twitter.com/' + name + '/status/' + post_id)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    #response.body.scrollTop = response.body.scrollHeight;\n",
    "    #print(response)\n",
    "    temp = response.read().decode('utf-8')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        temp = temp.split('data-aria-label-part=\"0\">')\n",
    "        length = len(temp)\n",
    "        if length > 2:\n",
    "            for i in range(1, length):      \n",
    "                text.append(temp[i].split('</p>',1)[0])\n",
    "                name_list.append(name)\n",
    "\n",
    "            final = pd.DataFrame(np.column_stack([name_list, text])\n",
    "                                 , columns = ['name', 'comment'])\n",
    "        else:\n",
    "            final = pd.DataFrame(np.column_stack([name, 'N/A'])\n",
    "                                 , columns = ['name', 'comment'])\n",
    "    except:\n",
    "        final = pd.DataFrame(np.column_stack([name, 'N/A'])\n",
    "                             , columns = ['name', 'comment'])\n",
    "        \n",
    "    return final\n",
    "\n",
    "\n",
    "def tw(url, num, row):\n",
    "    \n",
    "    try:\n",
    "        if url.rsplit('?',1)[1] != None:\n",
    "            url = url.rsplit('?',1)[0]\n",
    "    except:\n",
    "        pass\n",
    "    #try:\n",
    "    if url.rsplit('/',2)[1] == 'status':\n",
    "        try:\n",
    "            data = tw_api.get_status(id = url.rsplit('/',1)[1])\n",
    "            timeline = []\n",
    "            timeline.append(data)\n",
    "            result = post_stat(url, url.rsplit('/',3)[1], timeline, 1, row)\n",
    "        except:\n",
    "            result = pd.DataFrame(np.column_stack(['error', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', url]), \n",
    "                         columns = ['name','post_date','UpdateDate','view_count','like_count','comment_count','share_count', 'url']) \n",
    "            print('error in ' + url)\n",
    "            \n",
    "    else:\n",
    "        try:\n",
    "            name = url.split('https://twitter.com/')[1].split('/')[0]\n",
    "            user = tw_api.get_user(name)\n",
    "            timeline = tw_api.user_timeline(screen_name = name, count = num*3, include_rts = False, include_entities = True)\n",
    "            result = post_stat(url, user, timeline, num, row)\n",
    "        except:\n",
    "            result = pd.DataFrame(np.column_stack([row, 0, 0, -3, 0, 0, 0, 0, 0, 0, 0, 0]), \n",
    "                         columns = ['number','YT', 'IG', 'TW', 'FB', 'UpdateDate', 'View (YT)','View (IG)','YT SA','IG SA(Image)','IG SA(Video)','TW SA'])\n",
    "            result = result.groupby('number').mean()\n",
    "            print('error in ' + url)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INSTAGRAM\n",
    "\n",
    "def to_json(url):\n",
    "    data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return json.loads(data)\n",
    "\n",
    "def insta_channel_stat(url, num, row):\n",
    "    view_count = []\n",
    "    like_count = []\n",
    "    comment_count = []\n",
    "    sub_count = []\n",
    "    date = []\n",
    "    address = []\n",
    "    content_type = []\n",
    "    sa_i = []\n",
    "    sa_v = []\n",
    "    image_count = 0\n",
    "    video_count = 0\n",
    "    url_temp = url\n",
    "        \n",
    "    count = 0\n",
    "    url = url + \"?__a=1\" # ?__a=1 는 JSON으로 변환시켜줌\n",
    "     \n",
    "    try:\n",
    "        while count <5 and (image_count < num or video_count < num): # count는 페이지를 로드하는 숫자.\n",
    "            # the count clicks \"Load more page\" \n",
    "            if count  == 0: # first iteration\n",
    "\n",
    "                data = to_json(url)\n",
    "                sub = int(data['user']['followed_by']['count'])\n",
    "                \n",
    "                cursor = data['user']['media']['page_info']['end_cursor'] # 'Load more page' cursor. this allows to load more pages\n",
    "\n",
    "                for i in range(0,12): # The first page shows 12 pictures\n",
    "                    \n",
    "                    df = data['user']['media']['nodes'][i] # for each post\n",
    "                    \n",
    "                    if df['is_video'] == False and image_count < num:     \n",
    "                        like_count.append(int(df['likes']['count']))\n",
    "                        comment_count.append(int(df['comments']['count']))\n",
    "                        date.append(pd.to_datetime(df['date'], unit = 's'))\n",
    "                        address.append('https://www.instagram.com/p/' + df['code'])\n",
    "                        sub_count.append(sub)\n",
    "                        view_count.append(0)                       \n",
    "                        content_type.append(0)\n",
    "                        sa_i.append(int(df['likes']['count']) + int(df['comments']['count']))\n",
    "                        sa_v.append(0)\n",
    "                        image_count += 1\n",
    "\n",
    "                    elif df['is_video'] == True and video_count < num:\n",
    "                        like_count.append(int(df['likes']['count']))\n",
    "                        comment_count.append(int(df['comments']['count']))\n",
    "                        date.append(pd.to_datetime(df['date'], unit = 's'))\n",
    "                        address.append('https://www.instagram.com/p/' + df['code'])\n",
    "                        sub_count.append(sub)\n",
    "                        view_count.append(int(df['video_views']))\n",
    "                        content_type.append(1)\n",
    "                        sa_i.append(0)\n",
    "                        sa_v.append(int(df['likes']['count']) + int(df['comments']['count']))\n",
    "                        video_count += 1\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "\n",
    "            else:   \n",
    "                url = url + \"&max_id=\" + cursor # 'Load More pagge' 를 눌러서 load more posts \n",
    "                        # &max_id와 cursor를 사용해서 페이지를 더 당겨옴\n",
    "\n",
    "                data = to_json(url)\n",
    "\n",
    "                cursor = data['user']['media']['page_info']['end_cursor'] # iter through end cursor \n",
    "\n",
    "                for i in range(0,11):\n",
    "\n",
    "                    df = data['user']['media']['nodes'][i] # for each post\n",
    "                    \n",
    "                    if df['is_video'] == False and image_count < num:                      \n",
    "                        like_count.append(int(df['likes']['count']))\n",
    "                        comment_count.append(int(df['comments']['count']))\n",
    "                        date.append(pd.to_datetime(df['date'], unit = 's'))\n",
    "                        address.append('https://www.instagram.com/p/' + df['code'])\n",
    "                        sub_count.append(sub)\n",
    "                        view_count.append(0)                       \n",
    "                        content_type.append(0)\n",
    "                        sa_i.append(int(df['likes']['count']) + int(df['comments']['count']))\n",
    "                        sa_v.append(0)\n",
    "                        image_count += 1\n",
    "\n",
    "                    elif df['is_video'] == True and video_count < num:                    \n",
    "                        like_count.append(int(df['likes']['count']))\n",
    "                        comment_count.append(int(df['comments']['count']))\n",
    "                        date.append(pd.to_datetime(df['date'], unit = 's'))\n",
    "                        address.append('https://www.instagram.com/p/' + df['code'])\n",
    "                        sub_count.append(sub)\n",
    "                        view_count.append(int(df['video_views']))\n",
    "                        content_type.append(1)\n",
    "                        sa_i.append(0)\n",
    "                        sa_v.append(int(df['likes']['count']) + int(df['comments']['count']))\n",
    "                        video_count += 1\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            count += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    dump = [0] * (image_count + video_count)\n",
    "    Row = [row] * (image_count + video_count)\n",
    "    \n",
    "    \n",
    "    final = pd.DataFrame(np.column_stack([Row, dump, sub_count, dump, dump, dump, dump, view_count, dump, sa_i, sa_v, dump, content_type]), \n",
    "                         columns = ['number','YT', 'IG', 'TW', 'FB', 'UpdateDate', 'View (YT)','View (IG)','YT SA','IG SA(Image)','IG SA(Video)','TW SA', 'type'])\n",
    "    \n",
    "    # 이미지와 비디오를 각각 나누어 평균을 냄\n",
    "    final = final.groupby('type').mean()\n",
    "    # 나눈 후에 두 이미지와 비디오 그룹을 합침\n",
    "    \n",
    "    if len(final) == 2:\n",
    "        final['IG'].loc[0] /= 2\n",
    "        final['IG'].loc[1] /= 2\n",
    "    \n",
    "    final = final.groupby('number').sum()\n",
    "    \n",
    "    \n",
    "    return final\n",
    "\n",
    "def insta_single_stat(url):\n",
    "    url = url.split(\"?\")[0]\n",
    "    address = url\n",
    "\n",
    "    url = url + \"?__a=1\" # JSON으로 변환\n",
    "        #cursor = data['graphql']['shortcode_media']['edge_media_to_comment']['page_info']['end_cursor'] #cursor\n",
    "        \n",
    "    data = to_json(url)['graphql']['shortcode_media']\n",
    "    comment_count = int(data['edge_media_to_comment']['count'])        \n",
    "    like_count = int(data['edge_media_preview_like']['count'])\n",
    "    name = data['owner']['username']\n",
    "    try:\n",
    "        date = data['edge_media_to_comment']['edges'][0]['node']['created_at']\n",
    "        #print('created')\n",
    "    except:\n",
    "        date = data['taken_at_timestamp']\n",
    "        #print('timestamp')\n",
    "        \n",
    "    #print(type(date))\n",
    "    #print(date)\n",
    "    \n",
    "    date = str(pd.to_datetime(date, unit = 's'))[:10]\n",
    "    \n",
    "    #df['just_date'] = df['dates'].dt.date\n",
    "    \n",
    "    if data['is_video'] == True:\n",
    "        content_type = 'video'\n",
    "        view_count = int(data['video_view_count'])\n",
    "    else:\n",
    "        content_type = 'image'\n",
    "        view_count = 'N/A'     \n",
    "    \n",
    "    #print(type(date))\n",
    "    \n",
    "    final = pd.DataFrame(np.column_stack([name, date, '', view_count, like_count, comment_count, 'N/A', address]), \n",
    "                         columns = ['name','post_date','UpdateDate','view_count','like_count','comment_count','share_count', 'url']) \n",
    "   \n",
    "    return final\n",
    "\n",
    "def comment_ig(url):\n",
    "    \n",
    "    channel_url = url\n",
    "    url = url.split(\"?\")[0]\n",
    "\n",
    "    url = url + \"?__a=1\" # JSON으로 변환\n",
    "\n",
    "    data = to_json(url)\n",
    "    comment_text = data['graphql']['shortcode_media']['edge_media_to_comment']['edges']\n",
    "    name = data['graphql']['shortcode_media']['owner']['username']\n",
    "    \n",
    "    id_name = []\n",
    "    url_list = []\n",
    "    comment_date = []\n",
    "    author = []\n",
    "    text = []\n",
    "\n",
    "\n",
    "    for i in range(0,1):\n",
    "    # 댓글 저장\n",
    "        if i != 0:\n",
    "            cursor = data['graphql']['shortcode_media']['edge_media_to_comment']['page_info']['end_cursor']\n",
    "            temp_url = url + \"&max_id=\" + cursor\n",
    "            data = to_json(temp_url)\n",
    "            comment_text = data['graphql']['shortcode_media']['edge_media_to_comment']['edges']\n",
    "\n",
    "            for i in range(0, len(comment_text)):\n",
    "                text.append(comment_text[i]['node']['text']) # Append each comment\n",
    "                id_name.append(name)\n",
    "                url_list.append(channel_url)\n",
    "                comment_date.append(str(pd.to_datetime(comment_text[i]['node']['created_at'], unit = 's'))[:10])\n",
    "                author.append(comment_text[i]['node']['owner']['username'])\n",
    "\n",
    "        else:\n",
    "            data = to_json(url)\n",
    "            comment_text = data['graphql']['shortcode_media']['edge_media_to_comment']['edges']\n",
    "            for i in range(0, len(comment_text)):\n",
    "                text.append(comment_text[i]['node']['text']) # Append each comment       \n",
    "                id_name.append(name)\n",
    "                url_list.append(channel_url)\n",
    "                comment_date.append(str(pd.to_datetime(comment_text[i]['node']['created_at'], unit = 's'))[:10])\n",
    "                author.append(comment_text[i]['node']['owner']['username'])\n",
    "                \n",
    "\n",
    "    final = pd.DataFrame(np.column_stack([id_name, url_list, comment_date, author,text])\n",
    "                         , columns = ['id', 'url', 'date', 'author_name','comment'])\n",
    "        \n",
    "    return final\n",
    "\n",
    "def ig(url, num, row):\n",
    "    \n",
    "    try:\n",
    "        if url.rsplit('?',1)[1] != None:\n",
    "            url = url.rsplit('?',1)[0]            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if url.split('https://www.instagram.com/')[1].split('/')[0] == 'p':\n",
    "        try:\n",
    "            result = insta_single_stat(url)\n",
    "        except:\n",
    "            result = pd.DataFrame(np.column_stack(['error', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', url]), \n",
    "                         columns = ['name','post_date','UpdateDate','view_count','like_count','comment_count','share_count', 'url']) \n",
    "            print('error in ' + url)\n",
    "    else:\n",
    "        try:\n",
    "            result = insta_channel_stat(url, num, row)\n",
    "        except:\n",
    "            result = pd.DataFrame(np.column_stack([row, 0, -3, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \n",
    "                         columns = ['number','YT', 'IG', 'TW', 'FB', 'UpdateDate', 'View (YT)','View (IG)','YT SA','IG SA(Image)','IG SA(Video)','TW SA'])\n",
    "            result = result.groupby('number').mean()\n",
    "            print('error in ' + url)\n",
    "  \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUTUBE\n",
    "\n",
    "def channel(user_name, api_key, num):\n",
    "    # Get unique id for each user.\n",
    "    \n",
    "    parameters = {\n",
    "                      'forUsername': user_name,\n",
    "                      'part': 'contentDetails',\n",
    "                      \"maxResults\" : num,\n",
    "                      \"key\": api_key\n",
    "\n",
    "                      }\n",
    "    url = \"https://www.googleapis.com/youtube/v3/channels\"  # This is the url to extract the data from YouTUbe\n",
    "\n",
    "    try:\n",
    "        \n",
    "        page = requests.request(method=\"get\", url=url, params=parameters) # Get JSON data\n",
    "        j_results = json.loads(page.text) # Convert JSON to dictionary\n",
    "        return j_results['items'].pop()['contentDetails']['relatedPlaylists']['uploads'] # This returns the unique ID of the user\n",
    "\n",
    "    except:\n",
    "        parameters = {\n",
    "          'id': user_name, # id is required instead of forUsername\n",
    "          'part': 'contentDetails',\n",
    "          \"maxResults\" : num,\n",
    "          \"key\": api_key\n",
    "          }\n",
    "        \n",
    "        page = requests.request(method=\"get\", url=url, params=parameters)\n",
    "        j_results = json.loads(page.text)\n",
    "        return j_results['items'].pop()['contentDetails']['relatedPlaylists']['uploads']\n",
    "    \n",
    "def playlist(user_id, api_key, num):\n",
    "    \n",
    "    # Get playlists \n",
    "    \n",
    "    parameters = {\"part\": \"id\",\n",
    "              \"part\": \"snippet\",\n",
    "              'part': 'contentDetails',\n",
    "              \"playlistId\" : user_id,\n",
    "              \"maxResults\" : num,\n",
    "              \"key\": api_key\n",
    "              }\n",
    "\n",
    "    url = \"https://www.googleapis.com/youtube/v3/playlistItems\" # URL to get playlists\n",
    "    page = requests.request(method=\"get\", url=url, params=parameters) # Get JSON data\n",
    "    j_results = json.loads(page.text) # Convert JSON to dictionary\n",
    "\n",
    "    video_id = [] # Video id\n",
    "    video_date = [] # video published date\n",
    "    \n",
    "    for i in range(0,num):\n",
    "        \n",
    "        video_id.append(j_results['items'][i]['contentDetails'].get(\"videoId\")) # Get video id \n",
    "        video_date.append(j_results['items'][i]['contentDetails'].get(\"videoPublishedAt\")[:-14]) # Get video id \n",
    "\n",
    "    return video_id, video_date\n",
    "\n",
    "def video_stat(video_id, video_date, user_name, api_key, num, row):\n",
    "    \n",
    "    # id 가져옴\n",
    "    parameters2 = {\n",
    "                      'forUsername': user_name,\n",
    "                      'part': 'contentDetails',\n",
    "                      \"maxResults\" : num,\n",
    "                      \"key\": api_key\n",
    "\n",
    "                      }\n",
    "    url2 = \"https://www.googleapis.com/youtube/v3/channels\"  # This is the url to extract the data from YouTUbe\n",
    "\n",
    "    try:\n",
    "        \n",
    "        page2 = requests.request(method=\"get\", url=url2, params=parameters2) # Get JSON data\n",
    "        j_results2 = json.loads(page2.text) # Convert JSON to dictionary\n",
    "        user_id = j_results2['items'].pop()['id'] # This returns the unique ID of the user\n",
    "\n",
    "    except:\n",
    "        parameters2 = {\n",
    "          'id': user_name, # id is required instead of forUsername\n",
    "          'part': 'contentDetails',\n",
    "          \"maxResults\" : num,\n",
    "          \"key\": api_key\n",
    "          }\n",
    "        \n",
    "        page2 = requests.request(method=\"get\", url=url2, params=parameters2)\n",
    "        j_results2 = json.loads(page2.text)\n",
    "        user_id = j_results2['items'].pop()['id']\n",
    "    \n",
    "    # subscriber count 가져옴\n",
    "    request = urllib.request.Request(\"https://www.googleapis.com/youtube/v3/channels?part=statistics&id=\" + user_id + \"&key=\" + api_key)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    j = json.loads(response.read().decode('utf-8'))\n",
    "    sub = j['items'].pop()['statistics']['subscriberCount']\n",
    "    \n",
    "    # Get each video data\n",
    "    \n",
    "    parameters = {\"part\": \"id\",\n",
    "              'part': 'contentDetails',\n",
    "              'part': 'statistics',\n",
    "              \"id\" : \"\",\n",
    "              \"maxResults\" : num,\n",
    "              \"key\": api_key\n",
    "              }\n",
    "\n",
    "    url = \"https://www.googleapis.com/youtube/v3/videos\" # URL to get video detail\n",
    "#    page = requests.request(method=\"get\", url=url, params=parameters) # Get JSON \n",
    "#    j_results = json.loads(page.text) # Convert JSON to dictionary\n",
    "\n",
    "    view_count = [] # View Count list\n",
    "    like_count = [] # Like Count LIst\n",
    "    dislike_count = [] #Dislike Count List\n",
    "    favorite_count = [] # Favorite count list\n",
    "    comment_count = [] # comment count list\n",
    "    sa_count = []\n",
    "    address = []\n",
    "    sub_count = []\n",
    "    name = [user_name] * len(video_id) # Generate a list of repeating names of the user\n",
    "    dump = [0] * num\n",
    "    Row = [row] * num\n",
    "\n",
    "    for vi_id in video_id: # Loop through each video ID\n",
    "        parameters['id'] = vi_id # \n",
    "        page = requests.request(method=\"get\", url=url, params=parameters) # Get JSON\n",
    "        j_results = json.loads(page.text) # Convert JSON to dictionary\n",
    "        \n",
    "        j_results = j_results['items'].pop() # pop out the list\n",
    "        \n",
    "        # Append the each item to the corresponding lists       \n",
    "        view_count.append(int(j_results['statistics']['viewCount'])) \n",
    "        like_count.append(int(j_results['statistics']['likeCount']))\n",
    "        dislike_count.append(int(j_results['statistics']['dislikeCount']))\n",
    "        favorite_count.append(int(j_results['statistics']['favoriteCount']))\n",
    "        comment_count.append(int(j_results['statistics']['commentCount']))\n",
    "        address.append('https://www.youtube.com/watch?v=' + j_results['id'])\n",
    "        sa_count.append(int(j_results['statistics']['likeCount']) + int(j_results['statistics']['commentCount']))\n",
    "        sub_count.append(int(sub))\n",
    "        \n",
    "    # Create dataframe using the lists\n",
    "    final = pd.DataFrame(np.column_stack([Row, sub_count, dump, dump, dump, dump, view_count, dump, sa_count, dump, dump, dump]), \n",
    "                         columns = ['number','YT', 'IG', 'TW', 'FB', 'UpdateDate', 'View (YT)','View (IG)','YT SA','IG SA(Image)','IG SA(Video)','TW SA'])\n",
    "    \n",
    "    final = final.groupby('number').mean()\n",
    "    return final #return the dataframe\n",
    "\n",
    "def single_stat(url, api_key):\n",
    "    parameters = {\"part\": \"id\",\n",
    "              'part': 'contentDetails',\n",
    "              'part': 'statistics',\n",
    "              \"id\" : \"\",\n",
    "              \"maxResults\" : num,\n",
    "              \"key\": api_key\n",
    "              }\n",
    "        \n",
    "    parameters['id'] = url.split('=')[1]\n",
    "    url = \"https://www.googleapis.com/youtube/v3/videos?id=\"+ url.rsplit('=',1)[1] +\"&key=\" + api_key + \"&fields=items(id,snippet(channelTitle,publishedAt,channelId,title,categoryId),statistics)&part=snippet,statistics\"\n",
    "    page = requests.request(method=\"get\", url=url, params=parameters) # Get JSON\n",
    "    j_results = json.loads(page.text) # Convert JSON to dictionary\n",
    "    j_results = j_results['items'].pop() # pop out the list\n",
    "    \n",
    "    name = j_results['snippet']['channelTitle']\n",
    "    video_date = j_results['snippet']['publishedAt'][:-14]\n",
    "    view_count = j_results['statistics']['viewCount']\n",
    "    like_count = j_results['statistics']['likeCount']\n",
    "    dislike_count = j_results['statistics']['dislikeCount']\n",
    "    favorite_count = j_results['statistics']['favoriteCount']\n",
    "    comment_count = j_results['statistics']['commentCount']\n",
    "    address = 'https://www.youtube.com/watch?v=' + j_results['id']\n",
    "\n",
    "    final = pd.DataFrame(np.column_stack([name, video_date, '', view_count, like_count, comment_count, 'N/A', address]), \n",
    "                         columns = ['name','post_date','UpdateDate','view_count','like_count','comment_count','share_count', 'url']) \n",
    "   \n",
    "    return final #return the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comment_yt(video_url, api_key): # Video ID 와 api_key를 받음\n",
    "    url = \"https://www.googleapis.com/youtube/v3/commentThreads\" # youtube comment api\n",
    "    \n",
    "    try:\n",
    "        if video_url.rsplit('&t=', 1)[1] != None:\n",
    "                video_url = video_url.rsplit('&t=',1)[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if url.rsplit('&',1)[1][:7] == 'feature':\n",
    "            url = url.rsplit('&',1)[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if url.rsplit('/',1)[1] == 'featured' or url.rsplit('/',1)[1][:6] == 'videos' or url[-1] == '/':\n",
    "        url = url.rsplit('/',1)[0]\n",
    "    \n",
    "    video_id = video_url.split(\"watch?v=\")[1]\n",
    "    #print(video_id)\n",
    "    count = 0\n",
    "    comment_list = []\n",
    "    comment_date = []\n",
    "    video_id_name = []\n",
    "    author = []\n",
    "    url_list = []\n",
    "    while True:\n",
    "        parameters = {\n",
    "                     'part': 'snippet',\n",
    "                      \"videoId\" : video_id,\n",
    "                      \"key\": api_key,\n",
    "                        \"pageToken\":\"\"\n",
    "        }\n",
    "\n",
    "        # 첫번째 페이지는 로드가 필요 없어서 \"\" 로 유지\n",
    "        if count== 0:\n",
    "            parameters['pageToken'] = \"\"\n",
    "        else:\n",
    "            # After the first loop, grab cursor to scroll down more for more results\n",
    "            parameters['pageToken'] = cursor\n",
    "\n",
    "\n",
    "        count = 1 # Count는 첫번째 페이지인가 아닌가를 판단\n",
    "                # Count가 1이 되어서 첫번째 패이지가 아닌걸로 판단해서 Cursor를 붙힘\n",
    "\n",
    "        try:\n",
    "            # Load Page\n",
    "            page = requests.request(method=\"get\", url=url, params=parameters)\n",
    "            j_results = json.loads(page.text)  \n",
    "            # Loop through items for each comment\n",
    "            for i in range(0,len(j_results['items'])):\n",
    "           \n",
    "                # 작석자 ID\n",
    "                author.append(j_results['items'][i]['snippet']['topLevelComment']['snippet']['authorDisplayName'])\n",
    "                #댓글\n",
    "                comment_list.append(j_results['items'][i]['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "                #댓글 단 날짜\n",
    "                comment_date.append(j_results['items'][i]['snippet']['topLevelComment']['snippet']['updatedAt'])\n",
    "                video_id_name.append(video_id) # 비디오 ID \n",
    "                url_list.append(video_url)\n",
    "                \n",
    "            cursor = j_results['nextPageToken'] # Create Cursor\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    final = pd.DataFrame(np.column_stack([video_id_name, url_list, comment_date, author,comment_list])\n",
    "                         , columns = ['video_id', 'url', 'date', 'author_name','comment'])\n",
    "    \n",
    "    return final\n",
    "\n",
    "def comment_to_word(data): # def YouTube_comment의 return value를 사용\n",
    "    # Special Character모두 지우기\n",
    "    data['comment_special'] = data['comment'].map(lambda x: re.sub('[^a-zA-Z0-9 ]', '', x))\n",
    "    \n",
    "    # 문장을 한개의 list로 합침\n",
    "    text = \" \".join(data['comment_special'].str.lower()).split(\" \")\n",
    "    text_count = Counter(text) # 단어 숫자를 dictionary로 변환\n",
    "    #stop = set(stopwords.words('english')) # stopword \n",
    "    stop = {'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'as', 'at',\n",
    "            'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'd', 'did', 'didn',\n",
    "            'do', 'does', 'doesn', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'has',\n",
    "            'hasn', 'have', 'haven', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if',\n",
    "            'in', 'into', 'is', 'isn', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', 'more', 'most', 'mustn', 'my',\n",
    "            'myself', 'needn', 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "            'out', 'over', 'own', 're', 's', 'same', 'shan', 'she', 'should', 'shouldn', 'so', 'some', 'such', 't', 'than', 'that', 'the',\n",
    "            'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under',\n",
    "            'until', 'up', 've', 'very', 'was', 'wasn', 'we', 'were', 'weren', 'what', 'when', 'where', 'which', 'while', 'who', 'whom',\n",
    "            'why', 'will', 'with', 'won', 'wouldn', 'y', 'you', 'your', 'yours', 'yourself', 'yourselves'}\n",
    "    \n",
    "    del data['comment_special']\n",
    "    \n",
    "    # Stopword인지 아닌지 구분해서 stopword가 아니면 dictionary 만듬\n",
    "    text_final = {}\n",
    "    for key, value in text_count.items():\n",
    "        if key not in stop:\n",
    "            text_final[key] = value\n",
    "    # sort\n",
    "    text_final_2 = sorted(text_final.items(), reverse=True, key = operator.itemgetter(1))\n",
    "    # Create dataframe\n",
    "    final =pd.DataFrame.from_records(text_final_2,columns = ['word','number'])\n",
    "    final['number'] = pd.to_numeric(final['number'])\n",
    "    \n",
    "    name_list = []\n",
    "    for k in range(0,len(final)):\n",
    "        name_list.append((data['video_id'][0]))\n",
    "        \n",
    "    final['name'] = pd.DataFrame(np.column_stack([name_list]), \n",
    "                         columns = ['name'])\n",
    "        \n",
    "    final = final[['name', 'word', 'number']]\n",
    "    \n",
    "    return final\n",
    "\n",
    "def yt(url, api_key, num, row):\n",
    "\n",
    "    # https://www.youtube.com/user/DJCHETASOFFICIAL/adfads 같은 것들을 https://www.youtube.com/user/DJCHETASOFFICIAL로 바꿈\n",
    "    try:\n",
    "        if url.rsplit('&t=', 1)[1] != None:\n",
    "                url = url.rsplit('&t=',1)[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if url.rsplit('&',1)[1][:7] == 'feature':\n",
    "            url = url.rsplit('&',1)[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if url.rsplit('/',1)[1] == 'featured' or url.rsplit('/',1)[1][:6] == 'videos' or url[-1] == '/':\n",
    "        url = url.rsplit('/',1)[0]\n",
    "\n",
    "        \n",
    "#    user_url = [] # URL\n",
    "#    user_name = [] # User name ex) DJCHETASOFFICIAL, ZEDDVEVO ...\n",
    "#    user_type = [] # User type ex) user, channel, watch\n",
    "        \n",
    "    # /를 기준으로 2번 나눠줌 ex) https://www.youtube.com/user/DJCHETASOFFICIAL->'https://www.youtube.com', 'user', 'DJCHETASOFFICIAL'\n",
    "    user_url = url.rsplit('/',2) \n",
    "             \n",
    "    # Type이 user 나 channel일 경우 user_name과 user_type에 알맞은 값을 넣어준다\n",
    "    if user_url[1] == 'user' or user_url[1] == 'channel':\n",
    "        user_name = user_url[2]\n",
    "        user_type = user_url[1]        \n",
    "\n",
    "    # Type이 watch인 경우 name에는  - type에는 watch를 넣어준다 \n",
    "    elif user_url[2].split('?')[0] == 'watch':\n",
    "        user_name = ('-')\n",
    "        user_type = ('watch')       \n",
    "        \n",
    "    # user, channel, watch 다 아닐 경우 name과 type 모두에 undefine을 넣어준다\n",
    "    else:\n",
    "        user_name = ('undefine')\n",
    "        user_type = ('undefine')\n",
    "    \n",
    "    result = 0 #init\n",
    "    \n",
    "    if(user_type == 'user' or user_type == 'channel'):\n",
    "        try:\n",
    "            user_ids = channel(user_name, api_key, num)\n",
    "            video_id, video_date = playlist(user_ids, api_key, num)\n",
    "            result = video_stat(video_id,video_date, user_name, api_key, num, row)\n",
    "            return result\n",
    "        except:\n",
    "            result = pd.DataFrame(np.column_stack([row,-3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \n",
    "                         columns = ['number','YT', 'IG', 'TW', 'FB', 'UpdateDate', 'View (YT)','View (IG)','YT SA','IG SA(Image)','IG SA(Video)','TW SA'])\n",
    "            result = result.groupby('number').mean()\n",
    "            print('error in ' + url)\n",
    "        \n",
    "    elif(user_type == 'watch'):\n",
    "        try:\n",
    "            result = single_stat(url, api_key)\n",
    "        except:\n",
    "            result = pd.DataFrame(np.column_stack(['error', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', url]), \n",
    "                         columns = ['name','post_date','UpdateDate','view_count','like_count','comment_count','share_count', 'url'])\n",
    "            print('error in ' + url)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a number: 2\n",
      "https://www.youtube.com/user/BubbaWatsonGolf/videos\t1/6\n",
      "https://www.instagram.com/bubbawatson/?hl=en\t2/6\n",
      "https://twitter.com/bubbawatson\t3/6\n",
      "https://www.youtube.com/channel/UCtinbF-Q-fVthA0qrFQTgXQ\t4/6\n",
      "https://www.instagram.com/caseyneistat/\t5/6\n",
      "https://twitter.com/caseyneistat\t6/6\n",
      "done\n",
      "press close to exit2\n"
     ]
    }
   ],
   "source": [
    "# 'C:\\\\py_do_youtube\\\\input.xlsx'을 열어 활용할 수 있도록 세팅\n",
    "excel = win32com.client.Dispatch(\"Excel.Application\")\n",
    "#excel.Visible = False\n",
    "wb = excel.Workbooks.Open('C:\\\\SocialMediaData\\\\input.xlsx')\n",
    "ws = wb.ActiveSheet\n",
    "\n",
    "\n",
    "# 빈 셀이 나올 때 까지\n",
    "url = []\n",
    "url_num = 0\n",
    "row_offset = 1 # 첫 번째 url의 행 번호\n",
    "col_offset = 1 # 첫 번째 url의 열 번호\n",
    "# excel(row_offset,col_offset)부터 url 시작 url에 모든 excel의 모든 url을 저장해둠\n",
    "url.append(ws.Cells(url_num+row_offset,col_offset).Value)\n",
    "url.append(ws.Cells(url_num+row_offset,col_offset+1).Value)\n",
    "url.append(ws.Cells(url_num+row_offset,col_offset+2).Value)\n",
    "loop = True\n",
    "is_post = 0\n",
    "is_comment = 0\n",
    "\n",
    "while loop == True:\n",
    "     # 다음 셀을 확인하기 위해 num++를 해줌\n",
    "    url_num += 1\n",
    "    url.append(ws.Cells(url_num+row_offset,col_offset).Value)\n",
    "    url.append(ws.Cells(url_num+row_offset,col_offset+1).Value)\n",
    "    url.append(ws.Cells(url_num+row_offset,col_offset+2).Value)\n",
    "\n",
    "    if url[url_num * 3 + 1] == None and url[url_num * 3 + 2] == None:\n",
    "        is_post += 1\n",
    "    if url[url_num * 3] == None and url[url_num * 3 + 1] == None and url[url_num * 3 + 2] == None:\n",
    "        loop = False\n",
    "\n",
    "# 모든 url이 첫 번째 col에만 있다면 개별 포스팅 url이고 아니라면 채널 url이라고 판단\n",
    "if is_post != url_num:\n",
    "    num = int(input(\"Enter a number: \"))\n",
    "    is_comment = 'n'\n",
    "    \n",
    "else:\n",
    "    while is_comment != 'y' and is_comment != 'n':\n",
    "        is_comment = input(\"Do you want to extract COMMENT data? (y/n)\")\n",
    "    num = 5 # 아무 값이나 넣음\n",
    "    \n",
    "    \n",
    "    \n",
    "init = False\n",
    "############################################\n",
    "comment_youtube = []\n",
    "comment_analytics_youtube = []\n",
    "comment_instagram = []\n",
    "comment_twitter = []\n",
    "############################################\n",
    "for count in range(0,url_num*3):\n",
    "    # row는 몇번째 row의 url값인지 저장해두는 변수\n",
    "    row = int(count / 3) + 1\n",
    "\n",
    "    try:\n",
    "        if url[count].find('https://www.youtube.com') != -1:\n",
    "            platform = 'youtube'\n",
    "        elif url[count].find('https://twitter.com') != -1:\n",
    "            platform = 'twitter'\n",
    "        elif url[count].find('https://www.instagram.com') != -1:\n",
    "            platform = 'instagram'\n",
    "        else:\n",
    "            print('error: ' + url[count])\n",
    "            if is_post == url_num:\n",
    "                final = final.append(pd.DataFrame(np.column_stack(['error', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', url[count]]), \n",
    "                         columns = ['name','post_date','UpdateDate','view_count','like_count','comment_count','share_count', 'url']))\n",
    "            else:\n",
    "                final = final.append(pd.DataFrame(np.column_stack([row, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \n",
    "                         columns = ['number','YT', 'IG', 'TW', 'FB', 'UpdateDate', 'View (YT)','View (IG)','YT SA','IG SA(Image)','IG SA(Video)','TW SA']).groupby('number').mean())\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    if is_post != url_num:\n",
    "        print(url[count] + '\\t' + str(count + 1) + '/' + str(url_num * 3))\n",
    "    \n",
    "    else:\n",
    "        print(url[count] + '\\t' + str(int(count/3) + 1) + '/' + str(url_num))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 주소의 문제가 아닌 이유 모를 오류가 날 때가 있으니 에러의 경우 같은 작업을 한번 더 해봄\n",
    "    try:\n",
    "        if count == 0 or init == False:\n",
    "            init  = True        \n",
    "            if platform == 'youtube':\n",
    "                final = yt(url[count], yt_api_key, num, row)                   \n",
    "            elif platform == 'twitter':\n",
    "                final = tw(url[count], num, row)            \n",
    "            elif platform == 'instagram':\n",
    "                final = ig(url[count], num, row)                 \n",
    "            else:\n",
    "                init = False\n",
    "                continue\n",
    "        else:\n",
    "            if platform == 'youtube':           \n",
    "                final = final.append(yt(url[count], yt_api_key, num, row))                      \n",
    "            elif platform == 'twitter':\n",
    "                final = final.append(tw(url[count], num, row))                \n",
    "            elif platform == 'instagram':\n",
    "                final = final.append(ig(url[count], num, row))            \n",
    "            else:\n",
    "                continue\n",
    "    except:\n",
    "        if count == 0 or init == False:\n",
    "            init  = True        \n",
    "            if platform == 'youtube':\n",
    "                final = yt(url[count], yt_api_key, num, row)                   \n",
    "            elif platform == 'twitter':\n",
    "                final = tw(url[count], num, row)            \n",
    "            elif platform == 'instagram':\n",
    "                final = ig(url[count], num, row)                 \n",
    "            else:\n",
    "                init = False\n",
    "                continue\n",
    "        else:\n",
    "            if platform == 'youtube':           \n",
    "                final = final.append(yt(url[count], yt_api_key, num, row))                      \n",
    "            elif platform == 'twitter':\n",
    "                final = final.append(tw(url[count], num, row))                \n",
    "            elif platform == 'instagram':\n",
    "                final = final.append(ig(url[count], num, row))            \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    if is_comment == 'y':\n",
    "        try:\n",
    "            if platform == 'youtube':\n",
    "                if len(comment_youtube) == 0:\n",
    "                    save = comment_yt(url[count], yt_api_key)\n",
    "                    comment_youtube = save\n",
    "                    comment_analytics_youtube = comment_to_word(save)\n",
    "                else:\n",
    "                    save = comment_yt(url[count], yt_api_key)\n",
    "                    comment_youtube = comment_youtube.append(save)   \n",
    "                    comment_analytics_youtube = comment_analytics_youtube.append(comment_to_word(save))\n",
    "            elif platform == 'twitter':\n",
    "                if len(comment_twitter) == 0:\n",
    "                    comment_twitter = comment_tw(url[count])\n",
    "                else:\n",
    "                    comment_twitter = comment_twitter.append(comment_tw(url[count]))         \n",
    "            elif platform == 'instagram':      \n",
    "                if len(comment_instagram) == 0:\n",
    "                    comment_instagram = comment_ig(url[count])           \n",
    "                else:\n",
    "                    comment_instagram = comment_instagram.append(comment_ig(url[count]))       \n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    " #################################################################### \"\"\"      \n",
    "        \n",
    "        \n",
    "for col in final.columns:\n",
    "    final[col] = pd.to_numeric(final[col], errors = \"ignore\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "pd.set_option('max_rows', 1000)\n",
    "            \n",
    "try:           \n",
    "    group = final.groupby('number').sum()\n",
    "    group['UpdateDate'] = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    group['View (YT)'].astype(int)\n",
    "    group['View (IG)'].astype(int)\n",
    "    group['YT SA'].astype(int)\n",
    "    group['IG SA(Image)'].astype(int)\n",
    "    group['IG SA(Video)'].astype(int)\n",
    "    group['TW SA'].astype(int)\n",
    "    writer = pd.ExcelWriter('C:\\\\SocialMediaData\\\\output.xlsx', engine='xlsxwriter')\n",
    "    group.to_excel(writer, sheet_name='Sheet1')\n",
    "    print('done')\n",
    "except:\n",
    "    final['UpdateDate'] = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    writer = pd.ExcelWriter('C:\\\\SocialMediaData\\\\output.xlsx', engine='xlsxwriter')\n",
    "    final.to_excel(writer, sheet_name='Sheet1')\n",
    "    print('done')\n",
    "\n",
    "\n",
    "try:\n",
    "    comment_youtube.to_excel(writer, sheet_name='YT')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    comment_instagram.to_excel(writer, sheet_name='IG')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    comment_twitter.to_excel(writer, sheet_name='TW')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    comment_analytics_youtube.to_excel(writer, sheet_name='YT_WORD')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "writer.save()\n",
    "\n",
    "k=input(\"press close to exit\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
